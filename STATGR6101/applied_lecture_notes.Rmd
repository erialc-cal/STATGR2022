---
title: "Applied lecture notes"
output: html_document
date: "2022-10-03"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lecture 1: introduction to regressions
## Introduction
Statistical relationship between predictors $x$ (or input, independent, explanatory variable) and response variables $y$ (or output, dependent) resulting from a functional relation and random errors. 

For practical examples in the course, we load the following package containing data from the book "Extending the linear model with R" (J. Faraway). Details can be found here https://julianfaraway.github.io/faraway/LMR/. 
```{r pressure, echo=FALSE}
#install.packages("faraway")
#install.packages("HistData")
library(faraway, HistData)
library(ggplot2)
library(GGally)
```

## Regression to mediocrity
We try to predict children's height using parents' height. 
$childHeight = \alpha + \beta midparentHeight + \varepsilon $ 

```{r}
data(GaltonFamilies, package='HistData')
plot(childHeight ~ midparentHeight, GaltonFamilies, pch=16)
lmod <- lm(childHeight ~ midparentHeight, GaltonFamilies)
abline(lmod)
coef(lmod)
```
Now using Darwin's theory that a parent a sd taller than the mean is likely to have a child with same characteristics.
```{r}
(beta1 <- with(GaltonFamilies, sd(childHeight / sd(midparentHeight))))
(alpha1 <- with(GaltonFamilies, mean(childHeight)- beta1*mean(midparentHeight)))
```
We see that the black line from Galton's theory is regression to mediocrity whilst the Darwin theory shows inflated extremal values.

(Regression line in dotted lines, $y=\alpha_1 + \beta_1 x$ in full line.)
```{r}
plot(childHeight ~ midparentHeight, GaltonFamilies, pch=16, main='Children height vs mid-parent height', xlab='mid-parent height', ylab='children height')
abline(a=alpha1, b=beta1)
abline(coef(lmod), lty=5)

```
Why is there regression to mediocrity? Because there's only a smaller portion of people being a SD taller than the mean (assuming for ex a normal distribution), so there's a dilution because of the mean. Note that this doesn't mean that variation across the whole population diminishes because there's enough random fluctuations across all heights so that even though we say an individual is 1 standard deviation above average implies that his child can expect a less than 1 standard deviation above average, the total variability is kept. 

# Lecture 2: linear regression 

## Regression: response, covariates
### Simple linear regression

$$\forall i= 1, \dots, n: \ \ \ y_i = \alpha+ \beta x_i + \varepsilon_i $$
We can't fit a straight line to our data points with it going through all the points, Galton introduces the idiosyncratic error through residuals captured with $\varepsilon$. 

### Least square estimate 
Measure "closeness" through SSE (sum of squared errors): 
$$ SSE = \sum_i [y_i - (\alpha + \beta x_i)]^2 $$ 

Least squares estimate: 
$$ (\hat{\alpha}, \hat{\beta})= \arg \min_{a, b} \sum_i [y_i - (\alpha + \beta x_i)]^2 = SSE(a,b) $$
Ofc we can look at first order conditions. Alternatively, an intuition on fitting least squares is noting that $$\min SSE(a,b) = \sum_i [y_i - (\alpha + \beta x_i)]^2 =  \sum_i [y_i - \bar{y} - (\alpha + \beta (x_i-\bar{x})]^2 $$
We can set our intercept to 0 going through middle points. 
Then let the slope be $\hat{\beta}=\frac{\sum_i (y_i - \bar{y})(x_i - \bar{x})}{\sum_i (x_i - \bar{x})^2}$ and intercept $\hat{\alpha}= \bar{y}-\hat{\beta}\bar{x}$. 

Can rewrite $\hat{\beta}=\frac{Cov(x,y)}{Var(x)}= \rho_{x,y} \frac{s_y}{s_x}$ 
We can recognise that regression to the mean (mediocrity) if $\rho_{x,y}<1$.

Fitted values: $\hat{y}_i = \hat{\alpha}+\hat{\beta}x_i$ 
Fitted line: $(\hat{y}_i - \bar{y})=\hat{\beta}(x_i - \bar{x})$

###  Residuals' properties

- SSE is minimised.  \newline
- orthogonality properties : \newline 
1. $\sum_i \hat{\varepsilon}_i=0$ ie error is null on average \newline
2. $\sum_i x_i\hat{\varepsilon}_i=Cov(x, \hat{\varepsilon})=0$ (if it isn't null, means there's still correlation in $x$ that we could fit our data better) \newline
3. $\sum_i \hat{y}_i\hat{\varepsilon}_i=0$ (as a linear combination of 1. and 2.) 


### Optimality of LSE
Two reasons for choosing LSE are relying on its simplicity.

#### BLUE 
The LSE is the BLUE ie Best Linear Unbiased Estimator ie restricting ourselves to the class of linear estimators, this is the choice with best properties. 
Consider $b = \sum_i a_i(y_i - \bar{y})$. Want to minimise variance of the estimator assuming we constraint it to be unbiased. 

$\mathbb{E}_{\varepsilon}(y_i - \bar{y})=\beta (x_i - \bar{x})$
**Assumption:** $Var(\varepsilon_i)= \sigma^2$ \newline

- Bias $= \beta - \mathbb{E}_{\varepsilon}(b)$ \newline 
- Variance $= var_\varepsilon(b)= \sum_i (a_i-\bar{a})$ \newline 

Solving the following for BLUE = $\min_a \sum_i (a_i - \bar{a})^2$ subject to $\sum_i a_i(x_i - \bar{x}) = 1$ \newline 
Using Lagrange multiplier, first order condition writes $a_i = \frac{x_i-\bar{x}}{\sum_j (x_j - \bar{x})^2}$

#### Interpretation of fit

$$ \mathbb{E}[Y-(a+bX)]^2 = \mathbb{E}[Y-\mathbb{E}(Y|X)]^2 +  \mathbb{E}[\mathbb{E}(Y|X) - (a+bX)]^2 $$ 

This means LSE allows a linear approximation to $\mathbb{E}(Y|X)$. The difference with this and the original linear regression, is that by taking the conditional expectation, we average out the idiosyncratic error and approximate over the conditional mean which allows our design to be random as well. 

#### Examples
- Binary $X$ \newline 
Say $X \in \{0,1\}$
$$\mathbb{E}(Y|X=x)= \mu_1 \mathbb{1}(x=1) + \mu_0 \mathbb{1}(x=0)  = (\mu_1-\mu_0)x + \mu_0 $$ 

Control group being 0, treatment group 1. Intercept gives the baseline effect, slope the treatment effect. If $X$ is binary, LS always works, it delivers treatment and baseline effect directly. 

- Depends on both $\mathbb{E}(Y|X)$ and the distribution of $X$ as well ! 
Read Buja et al. 2019, Statistical Science on  https://arxiv.org/pdf/1404.1578.pdf.
Often times a linear model is not sufficient if we're not looking at local $x$. 

- Stein's Identity 
Assumption: **Assume** $X$ is Gaussian. 
*Theorem: If $X \sim \mathcal{N}(0,1)*, then $\mathbb{E}(Xg(X))= \mathbb{E}g'(X)$. 

Estimand of the slope: set $f(X)= \mathbb{E}(Y|X)$, we get $\mathbb{E}(Y-(a+bX))^2 \to \beta = \mathbb{E}(f'(X)) $ 

### Summary
For OLS estimate we have 
- explicit form: $\hat{\beta}=\frac{Cov(x,y)}{Var(x)}= \rho_{x,y} \frac{s_y}{s_x}$ 
- for residuals, they are uncorrelated with fitted value, covariate:  $\sum_i x_i\hat{\varepsilon}_i=Cov(x, \hat{\varepsilon})=0$ 
- interpretation:
    - treatment effect
    - linear approximation to conditional mean
    - slope: (Stein's theorem) averaged derivative under Gaussian


# Lecture 3: Inference
## Scope of statistical inference 
Inference needs assumptions. 
- Point estimation $\hat{\theta}(X)$
- Confidence interval $P(\hat{\theta}_L(X) \leq \theta \leq \hat{\theta}_U(X)) \geq 1 - \alpha$
- Hypothesis testing : reject $H_0$ if $X \in \mathcal{R}$, $H_0 : \theta=\theta_0$
  - Testing $\mathcal{R}^c = \{X: \hat{\theta}_L(X) \leq \theta_0 \leq \hat{\theta}_U(X)\}$ (dual of confidence interval)
  - Asymmetry: can reject but cannot *confirm*

## Inference for SLR 

### Interpretation 
*Assuming*
- Linearity $E(Y|X)=\alpha+\beta X$
- Homoscedasticity $Var(\varepsilon_i)=\sigma_0^2$
- Independence of errors $\varepsilon_i \perp \ve_j$
- Normality of errors $\varepsilon_i \sim N(0,\sigma_0^2)$

```{r}
summary(lmod)
```

*Quantities to interpret out of output*
- regression coefficients $\alpha$, $\beta$ as
    - point estimates: $\hat{\alpha}=\bar{y}-\hat{\beta}\bar{x}$ and $\hat{\beta}= \frac{\sum_i (x_i - \bar{x})(y_i - \bar{y})}{\sum_i (x_i - \bar{x})$2}
    - sampling distribution $\hat{\beta} \sim N(\beta, \frac{\sigma_0^2}{[\sum_i (x_i-\bar{x})^2]^{-1}})$, $\bar{y}\perp \hat{\beta}\bar{x}$ 
- error variance $\sigma_0^2$
    - error variance from residuals $r_i = y_i - \hat{y}_i$ to $\hat{\sigma}^2 = \frac{1}{n-2}\sum_i r_i^2$ 
    - residual variance $\hat{\sigma}^2 \sim \sigma_0^2 \chi_{n-2}^2/(n-2)$ note that $(\hat{\alpha},\hat{\beta})\perp \hat{\sigma}^2$
- prediction: $y_{new}=\alpha+\beta x_{new}+\varepsilon_{new}$

    
### Test of significance 
Under $H_0 : \beta = 0$, $\frac{\hat{\beta}}{\hat{\sigma}[\sum_i (x_i - \bar{x})^2]^{-1/2}} \sim t_{n-2}$
Under $H_0 : \alpha = 0$, $\frac{\hat{\alpha}}{\hat{\sigma}[\frac{1}{n}+\frac{\bar{x}^2}{\sum_i (x_i - \bar{x})^2}]^{1/2}}\sim t_{n-2}$

Remark: many problems with t-value, needs *normality* and *independence*. However, even when assumptions don't hold, can somehow make it work. 

## ANOVA and R2
### ANOVA
- Decomposition of variation: recall in Galton's example, we see variance introduced by parent's height and intrinsic variance 
![When $x$ is not known, the best predictor of $y$ is $\bar{y}$ and the variation is denoted by the dotted line. When $x$ is known, we can predict $y$ more accurately by the solid line. $R^2$ is related to the ratio of these two variances.
](/Users/h2jw/Documents/GitHub/R/STATGR6101/applied_lecture_notes_files/galton.png)

We use analysis of variance: 
$$ \underbrace{\sum_i(y_i - \bar{y})^2}_{SST} = \underbrace{\sum_i(y_i - \hat{y})^2}_{SSE} + \underbrace{\sum_i(\hat{y}_i - \bar{y})^2}_{SSR}  $$

- $SST$: total variation
- $SSE$: unexplained variation
- $SSR$: variation explained by regression

### Goodness of fit
Or $R^2$ the coefficient of determination to assess prediction power. $$R^2 = 1 - \frac{\sum_i (y_i - \hat{y_i})^2}{\sum_i (y_i - \bar{y})^2} = \frac{SSR}{SST}$$
Note that for SLR, $R^2 = \rho_{X,Y}^2$.

$R^2$ gives an idea of how much information our predictors give to our response: goodness of fit. However, there is no "threshold" value for a good $R^2$. 

![Four simulated
datasets where $R^2$ is about 0.65. The plot on the upper left is well-behaved for $R^2$. In the plot on the upper right, the residual variation is smaller than the first plot but the variation in $x$ is also smaller so $R^2$ is about the same. In the plot on the lower left, the fit looks strong except for a couple of outliers while on the lower right, the relationship is quadratic.](/Users/h2jw/Documents/GitHub/R/STATGR6101/applied_lecture_notes_files/R2_comp.png)

We can also rewrite F-test in terms of $SSE$, $SSR$: note that $SSE \sim \sigma^2 \chi_{n-2}^2$, $SSR \sim  \sigma^2 \chi_1^2$ with $SSE \perp SSR$. Then the F-test for $H_0 : \beta = 0$ :
$$ F = \frac{SSR}{SSE/(n-2)} \sim F_{1,n-2}$$
# Lecture 4: Prediction
*Recap* 
How to make sense of $R$ output:
- Parameter estimation
- ANOVA analysis of variance: $SST = SSE + SSR$
- Inference under assumptions:
  -linearity
  - homoscedasticity
  - independence of errors
  - normality of errors


## Scope of prediction
We observe a new set of observation not used to estimate $\hat{\beta}$: $y_{new}=\alpha + x_{new}\cdot \beta + \epsilon_{new}$. 

- Estimate: $\hat{y}_{new} = \hat{\alpha}+x_{new}\cdot \hat{\beta}$
- Confidence interval: to calculate CI, we need to compute the variance of our estimator 
\begin{equation}
var(\hat{y}_{new}|x, x_{new}) = var(\hat{\alpha}+ x_{new}\cdot \hat{\beta}|x,x_{new}) \\
= \sigma^2\big(\frac{1}{n}+\frac{(x_{new}-\bar{x})^2}{\sum_j (x_j - \bar{x})^2\big)
\end{equation}
- Sampling distribution: $\hat{y}_{new} \sim N\big(\alpha + x_{new}\cdot \beta, \sigma^2\big(\frac{1}{n}+\frac{(x_{new}-\bar{x})^2}{\sum_j (x_j - \bar{x})^2\big) \big)$and can infer the CI for mean of $y_{new}$: $C_q = [\hat{\alpha}+ x_{new}\cdot \hat{\beta} \plusminus t_{n-2, 1-q/2}\cdot \sqrt{\sigma^2\big(\frac{1}{n}+\frac{(x_{new}-\bar{x})^2}{\sum_j (x_j - \bar{x})^2\big)}] $

```{r}
newdata <- data.frame(midparentHeight=70)
predict(lmod, newdata, interval='none')
predict(lmod, newdata, interval='confidence')
predict(lmod, newdata, interval='predict')
```

## Assumptions 


# Lecture 5: Inference in linear models 

Assumption for inference :
Check residuals plots
- Normality: $\varepsilon_i \sim \mathcal{N}(0,1)$ 
- Homoscedasticity: $Var(\varepsilon_i)=\sigma_0^2$ under mild conditions (large sample size), can get a valid estimator even without homoscedasticity using SSR
  
- non linearity: assume model $E(Y|X)= \alpha + \beta X + \delta(X)$ where $\delta(X)$ models the non linearity. By assumption of LS, $E(\delta(X))=E(X\delta(X))=0$. Interpretation of our estimator changes slightly, however still possible in terms of variation of quantities with care. Inference is affected however, 
Let $Y_i = \alpha + \beta X_i + \delta(X_i) + \varepsilon_i$ and set $\hat{\varepsilon}_i = \delta(X_i) + \varepsilon_i$. Therefore: 
Then $\hat{\beta} \underset{d}{\rightarrow} N(\beta, \frac{\delta_{\hat{\varepsilon}}^2}{\sum_i (x_i - \bar{x})^2})$
Noting $\sigma^2 = \sigma_0 ^2 + E(\delta(X)^2)$ yields
$\hat{\beta} \underset{d}{\rightarrow} N(\beta, \frac{\sigma^2}{\sum_i (x_i - \bar{x})^2})$

Slight caveat, we assume here that $\sigma_0 ^2 + E(\delta(X)^2)$ means $\delta(X_i) + \varepsilon_i$ are uncorrelated. Moreover, we need to be careful of fixed or random design. If random design, no issue with heteroscedasticity since it averages out. If fixed design, we need to also take care of heteroscedasticity. 


- independence: $Cov(\varepsilon_i, \varepsilon_j)=0$
However, if residuals not uncorrelated, we have 
$Cov(\varepsilon_i, \varepsilon_j)= \sigma_{ij}$ then $Var(\hat{\beta})=\frac{\sum_{i,j} \sigma_{ij}(x_i - \bar{x})(x_j - \bar{x})}{(\sum_i (x_i - \bar{x})^2)^2}$
Central limit theorem only holds under *weak dependence* ! Independence is a very strong assumption, weakening it is possible however it's not possible to get rid of it contrary to heteroscedasticity for example. Usually, we do not get around it, but try to model the dependence or only keep the point estimate. 

Implications:
- point estimate is still valid
- standard error estimate is *invalid*
- *interpretation* different !

## Scatterplot and modelling
Data analysis: overview of the data
Scatterplots relevant when not too many variables. 
```{r, echo=FALSE}
library('datarium')
data(marketing, package='datarium')
plot(marketing)
```


### Multiple linear regression
```{r}
res.lm <- lm(sales ~ youtube + facebook + newspaper, data=marketing) 
summary(res.lm)
```


Equivalent to list all variables or to use . 


```{r}
res.lm <- lm(sales ~ ., data=marketing) 
summary(res.lm)
```

MLR is a generalisation of SLR.


\begin{equation*}  
y_i = \alpha + \beta_1 x_{i1}+\beta_2 x_{i2}+ \dots + \beta_p x_{ip} + \varepsilon_i
\end{equation*} 


Idea with MLR is to observe variations of our variable with a change of each covariate, when *all others are fixed*. 

Back to SLR : $Y = \alpha + \beta X + \varepsilon$, interpretation with confounders so writing $Y = Z + \varepsilon'$. This can be rewritten as $X = YZ + \varepsilon''$ where $Z$ is the matrix of confounders. And the interest of the regression depends on our identification of confounders. This is the backing behind the idea of identifying change with "other covariates fixed". 
MLR is a $p$-component estimation problem of $\beta$ (we can get rid of $\alpha$ by substracting the mean). Note that $y_i = x_i^T \beta + \varepsilon_i$ in MLR. 


Our model rewrites to : 
\begin{equation*}  
Y = X\beta + \varepsilon
\end{equation*} 

with $Y = (y_1, \dots, y_n)^T, \varepsilon = (\varepsilon_1, \dots, \varepsilon_n)^T \in \mathbb{R}^n, X = (x_1, \dots, x_n) \in \mathbb{R}^{n \times p}$ where $x_i = (x_{i1}, \dots, x_{ip})^T \in \mathbb{R}^p$ and $\beta = (\beta_1, \dots, \beta_p)^T \in \mathbb{R}^p$ 

### Least squares in MLR
notation: $||a||^2 = \sum_i a_i^2$ 

To estimate the LS solution, use the MLE for independent normal errors, then:

\begin{equation*} 
\hat{\beta} \in \arg \min_{b \in \mathbb{R}^p} ||Y-Xb||^{2}
\end{equation*}

We fit a p-hyperplane through our data in an n-dimensional space. We might have no solutions, ..., even infinite number of solutions. 

A useful trick is to rewrite the following:

\begin{eqnarray*}
||Y  - Xb||^2 & =  & (Y - Xb)^T(Y - Xb) \\
& = & (Y^T + b^T X^T)(Y-Xb) \\
& = & Y^T Y + b^T X^T X b - 2 Y^T X b  
\end{eqnarray*}

Gradient of SS : $\nabla_b ||Y - Xb||^2 = 2(X^T X b - X^T Y)=  0$


Which yields if $X$ is full rank, finally $\hat{\beta}=(X^{T} X)^{-1}(X^{T} Y)$.

## Geometrical interpretation

We want to picture our covariate space as the column space of $X$.
\begin{equation*} 
\mathcal{C}(X)= \{ \theta: \theta = Xb, b \in \mathbb{R}^p\} \\
\hat{\theta}= \arg \min_\theta \{ ||\theta - Xb ||^2 : b \in \mathbb{R}^p \}
\end{equation*} 
The linear model is the space spanned by the $p$-columns of X. The question we're asking ourselves while fitting our linear model is: What's the closest point to $Y$ on the linear space spanned by $X$ ? This means that fitting a lm is basically obtaining a projection of $Y$ on the spanned column space of $X$. 

Note: projection properties of $E(Y|X) = X\beta$. 


Check-out Bill Benter (how he refined a statistical model to perfect his gambling).

# Lecture 6: Diagnostics

*Recap*
Regression as a projection : $P_X := X(X^T X)^{-1}X^T$ the projection matrix or *hat matrix*. See that $P_X X = X$ and $P_Y = P_X Y = X(X^T X)^{-1}X^T Y= X\hat{\beta}= \hat{\theta}=\hat{Y}$ gives you the fitted values $\hat{Y}$. 

Notice the dimensions of the projection matrix: $P_X = \underbrace{\underbrace{X}_{n \times p}\underbrace{(X^T X)^{-1}}_{p\times p} \underbrace{X^T}_{p\times n}}_{n \times n}$. 

## Linear model vs simple linear regression

- I. Linearity: $y = X\beta + \varepsilon$
- II. Strict exogeneity: $E(\varepsilon|X)=0$ see fixed and random design
Note that this emplies the weaker assumption of simple linear models : $E(\varepsilon)= E(E(\varepsilon|X))=0$. Stronger part comes from the conditional. 
- III. No multicollinearity $P(rank(X)=p)=1$ for fixed and random design, our matrix is almost surely full column rank which enables us to invert $X^T X$. In geometrical terms, we span in a $rank(X)$-dimension hyperplane, so such a regression doesn't allow us to separate the different effects individually.
For fixed design, just assume column full rank. If this assumption is broken, inference on $\beta$ is impossible without additional assumptions. 
Caveat when random design like suppose we generate a p-dimensional gaussian vector with independent entries, it "suffices" to assume that it is full rank almost surely. 
- IV. Spherical errors: $Var(\varepsilon|X)=\sigma^2 I_n$ sums up homoscedasticity ($E(\varepsilon_i^2|X)=\sigma^2$) and uncorrelated errors ($E(\varepsilon_i \varepsilon_j |X)=0, \ i \neq j$). 
- V. Normality $\varepsilon|X \sim \mathcal{N}(0, \sigma^2 I_n)$. 

Note that reading any linear model from R, we have to  assume all these five assumptions. 

## Similarities with SLR: finite sample properties
Here are some finite sample properties. 


- Under assumptions I-III:
  - $E(\hat{\beta}|X)=\beta$
- Under I-IV:
  - $Var(\hat{\beta}|X)= \sigma^2(X^T X)^{-1}$ (careful here, adding constants or not in $X$ impacts the variance)
  - BLUE estimator (Gauss Markov)
  - $Cov(\hat{\beta}, \hat{\sigma}^2 |X)= 0$
  - $E(\hat{\sigma}^2|X)= \sigma^2$
- Under I-V:
  - Cramer Rao lower bound is achieved
  - $\hat{\beta}|X \sim \mathcal{N}(\beta, \sigma^2(X^T X)^{-1})$


Note that generalised linear models are *not* a simple generalisation of simple linear models. We will see later more extensively the differences between them. 

## T-test and F-test
- Have a hypothesis test $H_0 : \beta_j = b_j$.

Under assumption I-V,  $\hat{\beta} |X \sim \mathcal{N}(\beta, \sigma^2(X^T X)^{-1})$ allows  $\hat{\beta}_j - b_j|\{X, H_0\} \sim \mathcal{N}(0, [\sigma^2(X^T X)^{-1}]_{jj})$

Suppose $X$ doesn't have intercept, $(X^T X)$ behaves like a covariance-matrix of $X$ then how do we interpret $[(X^T X)^{-1}]_{jj}$? Bigger variance is less straightforward in capturing the effect of $X$, variance of $X_j$ could be big, but  $[(X^T X)^{-1}]_{jj}$ is small, so we have to be mindful, what really is interpretable is the inverse covariance matrix. 

T-test: $t = \frac{\hat{\beta}_j - b_j}{\sqrt{[\sigma^2(X^T X)^{-1}]_{jj}}}=\frac{\hat{\beta}_j - b_j}{SE(\hat{\beta_j})}$

When we're not interested in not just one coefficient but a linear combination of coefficients, we can use a contrast matrix $C$ and our hypothesis becomes $H_0 : \underbrace{C^T}_{1 \times p} \underbrace{\beta}_{p \times 1} = C_0$ (e.g. $c = \begin{pmatrix} 0 \\ \dots \\ 0 \\ 1 \\ \dots \\ 0\end{pmatrix}$). 


Still use t-test : $t = \frac{C^T\hat{\beta}_j - C_0}{\sqrt{\sigma^2C^T(X^T X)^{-1}C}}$



- For multiple coefficient simultaneaously zero, use F-test. Consider $H_0 : \underbrace{D}_{k \times p}\underbrace{\beta}_{p \times 1} = d$ for $D$ with $k \leq p$ rows and rank.

$$D\hat{\beta} \sim \mathcal{N}(\underbrace{D\beta}_{d}, \sigma^2 D(X^T X)^{-1}D^T)$$
Then for the following, we recognize a chi-square distribution (if $X \sim \mathcal{N}(0, \Sigma), X^T \Sigma^1 X \sim \chi_p^2$), in particular, taking in account the degrees of freedom: $(D\hat{\beta} - d)^T [\hat{\sigma}^2 D(X^T X)^{-1}D^T]^{-1} (D\hat{\beta} - d)$ follows a sort of chi-square. Total degrees of freedom: $k$

Hence, $\frac{1}{k}(D\hat{\beta} - d)^T [\hat{\sigma}^2 D(X^T X)^{-1}D^T]^{-1} (D\hat{\beta} - d) \overset{H_0}{\sim} F_{r, n-p}$ where $r = \frac{\hat{\sigma}^2}{\sigma^2}$.

Different interpretation using the following $\tilde{\beta}= \arg \min_{b} ||y-Xb||_2^2 $ subject to $Db=d$. Can be solved using the Lagrangian method. Gives us an alternative expression for F-test, $F = \frac{(\tilde{r}^T \tilde{r}-r^T r)/k}{r^T  /(n-p)} $ where $\tilde{r}= y-X\tilde{\beta}$. 

In this context, the residuals are better in the full model than the restricted residuals (because constraint). So the fit without constraint is always better, however, how much better is it ? The fit is better because no constraints gives us more degrees of freedom. We want to look at per degrees of freedom, how much improvement we get without the constraint. This way of writing F shows us that F is a reasonable way of checking how likely our hypothesis is. Be mindful of the scaling. 

## Influencial points
### Leverage
We define leverage using the hat matrix $H$: $\ell_i = h_{ii}= [X(X^T X)^{-1}X^T]_{ii}$. 

$\hat{Y}= HY \Rightarrow \hat{Y}_i = \sum_{j=1}^n h_{ij} Y_j$ so fitted values is a linear combination of all observations. We can argue that we want to leverage our responses to stabilise our fitted values (we don't want to dilute too much by spreading too big our leverage, but we also want to spread enough so that we have a coherent fit - e.g. expect impact of the ii-th value bigger than the ij-th). Suppose $h_{ii}$ is big, this means that to fit the i-th obversation, we're trying hard to not deviate from the i-th observation. This is the leverage. 

Recalling that $H^2 = H$, $\ell_i=\ell_i^2 +\sum_{j \neq i} P_{ij}^2 \in [0,1]$ can be viewed as a weight. We don't want $\ell_i$ to be too close to 0, would be underfitting, and in the opposite situation, $\ell_i = 1$ would indicate overfitting on the i-th observation.

### Intuition of leverage
Leverage can tell us how stable our prediction is. But it can also give us information on the variance of residuals. Assuming errors are uncorrelated and homoscedastic. 

For $r = y - X\hat{\beta}= y - \hat{y} = (I-H)y$, note that $y=My=\underbrace{MX\beta}_{=0}+M\varepsilon$, then $Var(r|X)=  \sigma^2(I_n - H)$ (see slides for calculation). Our residuals are correlated since $r= M\varepsilon$ with $M = I-H$ (residuals are linear combination of errors) - but doesn't necessarily mean the errors are, it makes it however very hard to check for the errors. To assess how correlated they are, we see that variance of individual residual $r_i = \sigma^2 (1-\ell_i)$. We want to have *high variance* of residuals. We want to make sure we are not pretending to estimating $\varepsilon_i$ which is impossible. In this case, we are always overfitting the errors, but we want to control how much using our residuals by controlling the variance of the residuals to be large enough. So if leverage is too close to 1, we have small residuals... 

Note that leverage is not dependent on observations but on our design matrix through $H$ !! 

### Example: simple linear regression
The leverage for SLR $y = \alpha + \beta x + \varepsilon$, then $h_{ii}= \frac{1}{n}+\frac{(x_i - \bar{x})^2}{\sum_{i=1}^n (x_i - \bar{x})^2}$, leverage only depends on the distance $|x_i - \bar{x}|$. For SLR, leverage = levier, the further you are from the center, the bigger your leverage is since moving a little is gonna shift much more. 


Note: For mid-term: redo calculation in the classes, read Faraway's book on linear regression, especially how to read and interpret outputs. 

# Lecture 7 : Explanations 
*Mid-term instructions* 
Goes to what we cover to Monday. 
Chapters 1-6 covered. Some less discussed but need to know, for ex:

- Chapter 2
  - QR decomposition
  - Gauss Markov Theorem

Can skip because will cover later but good to know

- Chapter 3
  - Bootstrap
  - Permutation tests
- Chapter 4 
  - Autoregression
- Chapter 5
  - Less straightforward to use it but read it as a complement
  
## Influential points (cont.)
### Leverage (cont.)
Why do we not want high-leverage (close to 1) ?
Because a leverage close to 1 implies if the observations are noisy that we're explaining noise which is impossible. 
Another way of seeing it, is that we have $p$-leverages, but we need to explain $n$ points. If one individual takes most of the leverage, we might suspect it's taking too much from the other observations. Another risk is to fit too much towards an outlier. Here, careful of outliers vs influential points. (-> think of leverage influence with a simple linear regression might suffice most of the time, big noise of a high leverage data point will have more of lever effect than big noise on a low leverage data point)

Suppose we have 100 data points with 2 with high leverage: fit isn't necessary wrong, but caution, refit with 98 data points and re-assess about the influential points (if nothing changes, nothing wrong with the fit, if changes a lot, wanna modify or model). 


## Diagnostics
### Standardized residuals
If we don't standard residuals, some obs. might stand out because of their high variance! We want to use the leverage of our point so that we still capture the variability, but scaling it to it's influence. 
*Studentized residual* $= \frac{r_i}{\sqrt{\hat{\sigma}^2}(1-\ell_i)}$ 

### Cook's distance
Suppose i'm interested in the $i$-th obs predicting $y_i$. I run lm for the whole dataset and get $\hat{y}_i$ as prediction. Run lm with dataset with $i$-th observation removed. Note here $ (\hat{y})_{-i})_j$ as our fitted response value obtained when excluding $i$. 
\begin{eqnarray*}
D_i & =  & \frac{\sum{j=1}^n(\hat{y}_j - (\hat{y})_{-i})_j)^2}{} \\
& = & \frac{r_i^2}{p \hat{\sigma}^2} \big[ \frac{h_{ii}}{(1-h_{ii})^2}\big]
\end{eqnarray*}

$D_i$ explains how much change we get in the residuals by removing the $i$-th value. Very interpretative but computationally expensive if we do all the regressions. *But* we can actually obtain the Cook distance using only one fit and the equivalent formulation with leverage!  

### Diagnostic plot: 

- checking for homoscedasticity, *Residuals vs Fitted*: For a non-linear model, we'll get a non-linear fit, but we still want our fit to be close to the gray line
- checking for outliers *Standardized residual vs fitted*/*Residuals vs leverage*: Outliers/influential points are labelled/in second plot, we want all points near 0
```{r, echo=FALSE}
plot(res.lm)
```
```{r, echo=FALSE}
Name <- row.names(marketing)
plot(cooks.distance(res.lm)) 
#text(order(cooks.distance(res.lm)[0:4]), row.names(Name[0:4]), cex=0.6, pos=4)
```


### Handle outliers
- Delete them: be careful, can be sometimes the right thing to do
- Change the model: outliers break a pattern, they may represent a class we're not fitting, might want to change our model to account for two or multiple behaviours/populations
- Robust linear regression: downsize the influence of outliers


## Partial correlation
### Two variable regression
Back to $Y = \beta X + \gamma Z + \varepsilon$, suppose $X, Z$ our two variables. 
Least square estimate 
\begin{equation*} 
\begin{pmatrix} \hat{\beta} \\ \hat{\gamma} \end{pmatrix} = \begin{pmatrix} X^T X & X^T Z \\ Z^T X & Z^T Z \end{pmatrix}^{-1 } \begin{pmatrix} X^T Y \\ X^T Y \end{pmatrix} 
\end{equation*}

Use block matrix inversion, note that $Z(Z^T Z)^{-1}Z^T$ is a "hat matrix" for regression $X$ over $Z$ in the top first of diagonal matrix, and the RSS in the bottom last. 

\begin{equation*}
\begin{pmatrix} X^T X & X^T Z \\ Z^T X & Z^T Z \end{pmatrix}^{-1 } = \begin{pmatrix}(X^T X) & \\ & \end{pmatrix} \times \begin{pmatrix}  &  \\  &  \end{pmatrix}
\end{equation*}

### Two-stage linear regression
We find the exact same result as previously. 
This is a very useful trick. Can be extended to multivariate case. 

### Partial correlation and pairwise correlation
See : https://en.wikipedia.org/wiki/Partial_correlation 
Do the computation using slides

If conditional mean is not linear, the conditional correlation is different from partial correlation. 

### Geometric interpretation 
We project $Y$ to the $Z$ direction, and the residuals are obtained as what left in $r_y$ and similarly for $X$. The partial correlation is the angle obtained between those two residuals. 

In practice, the simplest is to do a two-stage linear regression and get the correlation. 
Another way is to run linear regression of $Y$ on $(X,Z)$, however this is not gonna be enough, still need to run $X$ on $Z$ because the coefficient of  $Y$ on $(X,Z)$ is gonna be the partial correlation $\times$ a function of the coef of  $X$ on $Z$ (compute it by hand to see). 




#Lecture 8
Disclaimer: Exam will be in two parts. 

Recap:
- Leverage
- Standardized residual
- Cook's distance
- Partial correlation

## Omitted Variable Bias 
We are asking ourselves what influence has missing a variable $Z$ in our model. Suppose tue true model $Y = \rho^T X + \gamma^T Z + \varepsilon$ but we fit $Y = X\beta + \varepsilno$. 

Recall : 

$$\beta  =   \frac{Cov(X,Y)}{Var(X)} =  \frac{Cov(X,\rho^T X + \gamma^T Z)}{Var(X)} \\
  =  \rho^T + \underbrace_{\text{ommitted variable bias}{\gamma^T \frac{Cov(X,Z)}{Var(X)}}
$$



## Toy example: Palmer station penguin data

```{r, echo=FALSE}
library(palmerpenguins)
data(package = 'palmerpenguins')
```

```{r}
head(penguins)
plot(penguins$bill_length_mm, penguins$bill_depth_mm, pch=19)
```

```{r}
lin_reg <- lm(bill_depth_mm ~bill_length_mm, data= penguins)
summary(lin_reg)
```

This looks "fine" in numbers... What is the issue here ? We're confronted to a *Simpson paradox*. 

What about our diagnostics ? The residuals vs fitted are slightly worrying, the fit is not perfect but nonetheless the linear relationship seems to be reasonable. 
```{r}
plot(penguins$bill_length_mm, penguins$bill_depth_mm, pch=19, main="Regression of bill length on bill depth of penguins")
abline(coef(lin_reg))
plot(lin_reg)
```

How do we interpret this result ? The longer the beak, the shallower it is. This sounds silly. We can sense the problem of our regression on the residual vs fitted plot. There are two stacks of point around the $x$s, we can see two or more "clumps"... What is missing is the variable of species ! 

```{r}
lin_1 <- lm(bill_depth_mm ~bill_length_mm: species, data= penguins)
summary(lin_1)
plot(lin_1)
```
```{r}
plot(penguins$bill_length_mm, penguins$bill_depth_mm, col= penguins$species, pch=19, main="Regression of bill length on bill depth of penguins")
a = 8.5628281
Adelie=0.2518668
Chinstrap=0.2019567
Gentoo = 0.2518668
abline(a= c(a,a,a), b= c(Adelie, Chinstrap, Gentoo), col=c('black', 'lightpink','green'))
```

```{r}
ggplot(penguins,aes(bill_length_mm, bill_depth_mm, colour= penguins$species)) + geom_point() + geom_smooth(method = "lm", se=FALSE)
```

For each species, the correlation of those two variables is positive, but for the clumped model, it is negative (which is not really interpretable)!


## Bootstrap
### Jacknife estimate
Robust inference under resampling ? 

If we look at $\bar{X}$ from $X_1, \dots, X_n$, we need to make assumption about the distribution 
Now, we replicate this but leaving one out to have not only one estimate but a population of estimate: $\bar{X}_{(-i)}= \frac{1}{n-1}\sum_{j\neq i} X_i$. We don't need an assumption on the distribution of those objects now, we now have $n$ of these objects and we can take as estimate $\bar{\bar{X}} = \frac{1}{n}\sum \bar{X}_{(-i)}$. 

Variance? Note that all $\bar{X}_{(-i)}$ are highly correlated, taking $\frac{1}{n-1}\sum (\bar{X}_{(-i)}-\bar{\bar{X}})^2$ won't yield the right amount. Instead, take $\frac{n}{n-1} (\bar{X}_{(-i)}-\bar{\bar{X}})^2 = \frac{1}{n(n-1)} \sum (X_i - \bar{X})^2$. 

Now let's take the model $Y = X^T \beta + \epsilon$ with $(X_i, Y_i)_{i=1, \dots, n}$. We compute likewise $\hat{\beta}_{(-i)}$ by regression $Y$ on $X_{(-i)}$. Computations yield $\hat{\beta}_{(-i)} = (X_{(-i)}^TX_{(-i)})^{-1} X_{(-i)}^T Y_{(-i)}$.
A simple way to retrieve this result with intuition is using the "leave-one-out" lemma.that gives $\hat{\beta}_{(-i)}= (X^T X)^{-1}X^T \hat{y}_{(-i)}$


Variance? In the specific case where we already have $\hat{\beta}$ to plug in our leave-one-out, $$\sum_{i=1}^n (\hat{\beta}_{(-i)}-\hat{\beta})(\hat{\beta}_{(-i)}-\hat{\beta})^T 
=(X^TX)^{-1}X^T(\sum (\hat{y}_{(-j)}-(\frac{n-1}{n}y+\frac{1}{n}X^T \hat{\beta})(\hat{y}_{(-j)}-(\frac{n-1}{n}y+\frac{1}{n}X^T \hat{\beta})^T)) \\ 
= (X^T X)X^T D(r_i^2) X(X^T X)^{-1} $$

Downsize: need either a number of regressions... if not in this specific case (in which we can use our closed form formula). 
Plus: resample eases inference.

### Confidence interval
Point estimate might not be changed by lack of assumptions. However, you need the assumptions for confidence interval. 


### Case resampling
default method of resampling. 
```{r}
fit <- lm(sales ~., data=marketing)
library(car)
boot.model <- Boot(fit, method='case', R=10000)
hist(boot.model, layout=c(2,2))
```
Important to know how to interpret this. Even bootstrap CI aren't necessarily precise. However, can check if our previous CI is satisfactory by comparing with the bootstrap. 

Question? the QQ-plot indicates normality not held. However the resampling yields a beta estimate that is normal. One possible explanation is CLT (large dataset). 


### Residual resampling (parametric boostrap)
Suppose I fit a lm, i'm assuming the truth is a lm, and normality etc. We can turn things around: i can generate data from my fitted data: $\tilde{Y} = X^T \hat{\beta} + \hat{\epsilon}$. We have two caveats, choice for $X$ and for $\hat{\epsilon}$. For $X$ we keep the design matrix. We choose to resample $\hat{\epsilon}$ along the residuals, yielding $\tilde{\epsilon}$
```{r}
boot.model2 <- Boot(fit, method="residual", R=1000)
hist(boot.model2, layout=c(2,2))
```

### Bootstrap with other stats 
<!-- ```{r}
library(boot)
my.data <- marketing %>% mutate( fitted= fitted(fit), resid= resid(fit))
# We are adding two variables 
my.stat <-


boot.model <- boot(my.data, my.stat, R=1000) 
```--> 

