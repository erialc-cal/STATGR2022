---
title: "Theoretical Statistics"
output: html_notebook
---

# Question 1

Posteriors for $i = 1,2$:

$\begin{eqnarray*} p(p_i|y_i) & \propto & f(y_i|p_i) \pi_i(p_i) \\  & \propto & p_i^{y_i}(1-p_i)^{n_i- y_i} p_i^{\alpha_i}(1-p_i)^{\beta_i -1} \\ & \sim & Beta(y_i+\alpha_i, n_i - y_i + \beta_i) \end{eqnarray*}$

Parameter choice:

```{r}
set.seed(123)
n=100000
alpha1 = 1
beta1 = 1
alpha2 = 1
beta2 = 1
n1 = 10
y1 = 6
n2 = 20
y2 = 10
```

Sampling $p_1$ and $p_2$ to get the quantiles by simulation:

```{r}
p1 <- rbeta(n, y1+alpha1, n1 - y1 + beta1, ncp = 0)
p2 <- rbeta(n, y2+alpha2, n2 - y2 + beta2, ncp = 0)
q1 <- quantile(p1,probs = c(0.025, 0.975))
q2 <- quantile(p2,probs = c(0.025, 0.975))
par(mfrow=c(2,2))
hist(p1)
hist(p2)
```
```{r}
q1-q2
```

Simulation yields the credible interval at level 95\% : $C_{95\%} = [0.0106, 0.1326]$ for $\hat{p}_1-\hat{p_2}$. 

Can check with quantile bounds from Beta distribution:
```{r}
qbeta(c(0.025, 0.975),y1+alpha1, n1 - y1 + beta1)-qbeta(c(0.025, 0.975),y2+alpha2, n2 - y2 + beta2)
```

Want to estimate posterior probability of $p_1 > p_2$ ie $P(p_1 - p_2 > 0|\{y_1, y_2\})$. We have sampled $p_1$ and $p_2$ hence $\hat{P}(p_1 - p_2 > 0|\{y_1, y_2\}) = 0.68613$.

```{r}
mean(p1 > p2)
```

# Question 2
We want to sample standard normal rv using exponential rv. 
```{r}
rnorm_exp_AR <- function(n){
  x <- rep(NA,n)
  for(i in 1:n){
    y <- rexp(1, rate=1)
    u <- runif(1, 0, 1)
    rhoY <- exp(-1/2)*exp(-0.5*y**2+y)
    if (u < rhoY){
      p <- rbinom(1,1, p=0.5)
      if (p == 0){
        x[i]<- -y
      }
      else{
        x[i]<- y
      }
    }
  }    
  return(x)
}
```
Can see acceptance-rejection rate: 
```{r}
test<- rnorm_exp_AR(n)
sum(test != "NA")/n 
```

And our sampling:  
```{r}
hist(as.numeric(test[which(test !="NA")]), freq=FALSE, main="Sampled distribution for N(0,1)", xlab="samples")
x <- seq(-4,4,by = .2)
curve(dnorm(x,0,1), col="red", add=TRUE)
```


# Question 3

Note that $N(0,1)$ is invariant by rotation. Linking this symmetry property and the fact that $\mathcal{S}$ is symmetric, can sample uniformly on $\mathcal{S} = \{(x_1, \dots, x_p): \sum_i x_i^2 = 1 \}$ if we sample renormalized standard normal for unit vectors.

Proposal:

- sample a standard normal $X \sim N(0,I_p)$
- renormalise the standard normal $U := \frac{X}{||X||_2}$ 

Then $U \sim \mathcal{U}(\mathcal{S})$. To see this, see that if $X \sim N(0,I)$, for any $O$ orthogonal, $OX \sim N(0,I)$ which justifies the invariance with rotations and then $U$ is also invariant through rotation. Also, renormalising yields $||U||_2 = 1$ so $U \in \mathcal{S}$ as required. 


```{r}
# sample a p-vector on S
p_shell <- function(p){
  u <- rnorm(p,0,1)
  x <- u/(norm(u, type='2'))
  return(x)
}
```

Check if this works for large $p=1 000 000$: 
```{r}
large_p_test <- p_shell(1000000)
norm(large_p_test, type='2')
```

Now we want to estimate $E_\mathcal{S}(|X_1|)$ for $p=100,200,300$. 
```{r}
# Sample X_1 on S n times 
p <- c(100,200,300)
est_X <- c(1,2,3)
for (i in 1:3){
  est_X[i] <- mean(replicate(n,abs(p_shell(p[i])[1])))
}
```

Which gives us the following estimates for each $p$:
```{r}
est_df <- t(as.data.frame(est_X))
colnames(est_df) <- p
est_df
```


Now, we want to sample uniformly from the $p$-dimensional ball $\mathcal{B}= = \{(x_1, \dots, x_p): \sum_i x_i^2 \leq 1\}$.
We want to proceed similarly as before. For the radius to vary between 0 and 1, take $R \sim \mathcal{U}(0,1)$ and $Z \sim N(0,I_p)$. Then claim that $X := U^{1/p}\frac{Z}{||Z||_2}$ is uniform on $\mathcal{B}$. The rotational invariance is preserved and note that now $||X||_2 = ||U^{1/p}\frac{Z}{||Z||_2}||_2 = ||U^{1/p}||_2 \leq 1$. So it is on $\mathcal{B}$, and moreover, we are picking $U$ from uniform on the radius length: see that in polar coordinates for a 2-ball, $r, \theta$ are the parameters for the disk with radius $r$ and $\theta$ for angles. Note that the joint density on the disk yields $f(r, \theta)=\frac{1}{2\pi} r$ for $r \in (0,1)$ and $\theta \in (0, 2\pi)$ such that for $R \in (0,1)$ since $U \sim \mathcal{U}(0,1)$, 
$$U = F(R)= \int_0^R \int_0^{2\pi} \frac{1}{2\pi} r dr d\theta  = R^2$$ 
and $R = F(R)^{1/2} = U^{1/2}$ is the contribution on the radius. Can show this holds in higher dimensions similarly, using joint density for dimension $p$ by adding angle coordinates $f(r, \theta_1, \dots, \theta_{p-1}) \propto r^{p-1}$ with each angle contribution adding up $r$, hence $U = F(R) \propto \int_0^R \int \cdots \int dr r^{p-1} d\theta_1 \cdots d\theta_{p-1} \propto R^p$ hence $R = U^{1/p}$.


```{r}
# sample a p-vector on B
p_ball <- function(p){
  u <- runif(1,0,1)
  rn <- rnorm(p,0,1)
  x <- u**(1/p)*rn/(norm(rn, type='2'))
  return(x)
}
```

Check if this works for large $p=1 000 000$: 
```{r}
large_p_test2 <- p_ball(1000000)
norm(large_p_test2, type='2')
```


Now we want to estimate $E_\mathcal{B}(|X_1|)$ for $p=100,200,300$. 
```{r}
# Sample X_1 on B n times
p <- c(100,200,300)
est_X2 <- c(1,2,3)
for (i in 1:3){
  est_X2[i] <- mean(replicate(n,abs(p_ball(p[i])[1])))
}

est_df2 <- t(as.data.frame(est_X2))
colnames(est_df2) <- p
est_df2
```





